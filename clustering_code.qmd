---
title: "Base R code"
format: html
editor: source
---

This document shows Base R code for the R learning project. If there are any differences compared to the tidyverse-workflow in the main document, those are described below.

## Environment Preparation

```{r}
#| eval: false
#| freeze: true
library(reticulate)
library(keras)
library(tensorflow)
library(dplyr)
library(factoextra)
library(cluster)
#library(imager) # not loaded yet, because of conflicts
```

```{r}
#| eval: false
#| freeze: true
use_condaenv("condatascience") # on my windows machine
myPy <- py_config()$python
myPy
```

```{r}
#| eval: false
#| freeze: true
tf$constant("Hello Tensorflow!")
```

```{r}
#| eval: false
#| freeze: true
get_time <- function(){
  # returns time-string without special characters
  myTime <- Sys.time()
  myTime <- gsub(":","_",myTime)
  myTime <- gsub(" ","_",myTime)
  myTime <- gsub("-","_",myTime)
  return (myTime)
  }
```

## Keras and image feature extraction

```{r}
#| eval: false
#| freeze: true
get_features <- function(image_file,myModel){
  # takes: image file and model
  # returns: feature vector for the provided image
  # description:
  # the image is loaded with color and scaled to 224x224 px.
  # In the next step the image is converted to an array and this array is prepared for prediction on the provided model
  img <- image_load(image_file, grayscale=FALSE,target_size = c(224,224))
  img_array <- image_to_array(img)
  reshaped_image_array <- array_reshape(img_array,c(1,dim(img_array)))
  prepro_img <- imagenet_preprocess_input(reshaped_image_array)
  features <- myModel |> predict(prepro_img)
  return(features)
}
```

```{r}
#| eval: false
#| freeze: true
# Load model
model <- application_vgg16(weights="imagenet",include_top=TRUE)
# Change Output to second last layer to access the feature map instead of classification result.
output <- model$layers[[length(model$layers)-1]]$output
model <- keras_model(inputs=model$input, outputs=output)
```

```{r}
#| eval: false
#| freeze: true
# Switch to toggle between a prototype flower-images and the actual images to be clustered
flower_data <- FALSE
if (flower_data){
  myPath <- "flowers"
  print("Flower Data loaded.")
} else {
  myPath <- "weapons"
  print("Weapon Data loaded.")
}
# Actual loading of files to a list
myFiles <- list.files(myPath)
```

```{r}
#| eval: false
#| freeze: true
# empty list to hold extracted features per image
myFeatures <- list()

# fill list with features by looping through file list and calling the function get_features
for (img_file in myFiles){
  # create path and call function
  path <- paste(myPath,"/",img_file,sep="")
  feat <- get_features(path,model)
  # append List with features
  len <- length(myFeatures)
  myFeatures[[len+1]]<-feat
}

x <- matrix(unlist(myFeatures), ncol=4096, byrow=TRUE) 
```

```{r}
#| eval: false
#| freeze: true
dim(x)
```

## PCA

In comparison to the recipe the pca analysis is performed with prcomp(). According to [Statology.org (2020)](https://www.statology.org/principal-components-analysis-in-r/) the eigenvectors should be multiplied by -1, because R eigenvectors are negative by default.

```{r}
#| eval: false
#| freeze: true
pca_res <- prcomp(x,rank=100)
pca_res$x <- -1*pca_res$x
```

```{r}
#| eval: false
#| freeze: true
# transform myFiles list to a single column dataframe
file_names <- as.data.frame(myFiles)
  
# create a dataframe with prcomp per row
file_features <- as.data.frame(pca_res$x)

# append both dataframes to one (no join due to missing parameter)
# bind_cols is loaded from dplyr
df_pca <- bind_cols(file_names,file_features)

write_csv(df_pca,paste(get_time(),"_","pcaResults_",myPath,".csv",sep=""))
```

The output can be used to plot the elbow curve. There is a special library called factoextra for this.

```{r}
#| eval: false
#| freeze: true
fviz_nbclust(file_features,kmeans,method="wss",k.max=66)
```

## Clustering

```{r}
#| eval: false
#| freeze: true
km <- kmeans(pca_res$x,centers=10,nstart=3)
```

```{r}
#| eval: false
#| freeze: true
# map cluster and file_name
clusters <- as.data.frame(km[1])
clusters <- clusters |>
  rename(".cluster" = "cluster") |>
  mutate(".cluster" = paste("Cluster_",.cluster,sep=""))
#names(clusters) <- ".cluster"
clustered_files <- bind_cols(file_names,clusters)
clustered_files

# export clusters_dataframe to csv-file
write_csv(clustered_files,paste("noRecipe_",myPath,"_",get_time(),"_","cluster.csv",sep=""))
```

The kmeans object provides further arguments to receive details like the cluster assignments, but also the cluster sizes.

```{r}
#| eval: false
#| freeze: true
km$size
```
